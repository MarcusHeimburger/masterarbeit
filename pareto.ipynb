{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc, recall_score, precision_score\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# LÃ¤dt Daten\n",
    "usu_graph = torch.load('Graph/usu_graph.pth')\n",
    "usu_graph_oversampled_easy = torch.load('Graph/syntetic_data.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(25, 4)\n",
      "  (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (bn3): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(25, 4)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout1 = torch.nn.Dropout(0.5)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(2)\n",
    "        self.dropout3 = torch.nn.Dropout(0.5)\n",
    "        self.classifier = Linear(2, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.bn1(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout1(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.bn2(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout2(h)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self.bn3(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout3(h)\n",
    "\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 0, Loss: 0.9451559782028198\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 1, Loss: 0.9339910745620728\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 2, Loss: 0.9260878562927246\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 3, Loss: 0.9077959656715393\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 4, Loss: 0.8949931859970093\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 5, Loss: 0.8849787712097168\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 6, Loss: 0.8755815029144287\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 7, Loss: 0.8649537563323975\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 8, Loss: 0.8592509627342224\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 9, Loss: 0.8498400449752808\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 10, Loss: 0.8405616879463196\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 11, Loss: 0.8331317901611328\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 12, Loss: 0.8191952109336853\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 13, Loss: 0.8127669095993042\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 14, Loss: 0.7983057498931885\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 15, Loss: 0.7955847382545471\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 16, Loss: 0.7840031981468201\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 17, Loss: 0.7748575806617737\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 18, Loss: 0.7645233273506165\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 19, Loss: 0.760486364364624\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 20, Loss: 0.7463226914405823\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 21, Loss: 0.7413080930709839\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 22, Loss: 0.7297562956809998\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 23, Loss: 0.7228385210037231\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 24, Loss: 0.710679292678833\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 25, Loss: 0.7073660492897034\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 26, Loss: 0.6974738836288452\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 27, Loss: 0.6854221224784851\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 28, Loss: 0.6809446811676025\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 29, Loss: 0.6710963845252991\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 30, Loss: 0.6726298928260803\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 31, Loss: 0.6603862643241882\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 32, Loss: 0.6525924801826477\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 33, Loss: 0.6474714875221252\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 34, Loss: 0.6514859199523926\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 35, Loss: 0.6349793672561646\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 36, Loss: 0.63369220495224\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 37, Loss: 0.6280792951583862\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 38, Loss: 0.619986891746521\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 39, Loss: 0.6106069087982178\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 40, Loss: 0.616296648979187\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 41, Loss: 0.6131098866462708\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 42, Loss: 0.6009458899497986\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 43, Loss: 0.6025345921516418\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 44, Loss: 0.589739978313446\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 45, Loss: 0.5929201245307922\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 46, Loss: 0.5841706395149231\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 47, Loss: 0.6119787693023682\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 48, Loss: 0.5833016037940979\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 49, Loss: 0.5809698104858398\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 50, Loss: 0.5849145650863647\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 51, Loss: 0.5761037468910217\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 52, Loss: 0.5711724758148193\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 53, Loss: 0.5781532526016235\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 54, Loss: 0.5706735849380493\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 55, Loss: 0.5715283155441284\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 56, Loss: 0.5693480968475342\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 57, Loss: 0.569870114326477\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 58, Loss: 0.5692561864852905\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 59, Loss: 0.5622370839118958\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 60, Loss: 0.5635805726051331\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 61, Loss: 0.5631416440010071\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 62, Loss: 0.5727682113647461\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 63, Loss: 0.567432701587677\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 64, Loss: 0.5664543509483337\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 65, Loss: 0.5677257180213928\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 66, Loss: 0.5600593686103821\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 67, Loss: 0.558536946773529\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 68, Loss: 0.5603164434432983\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 69, Loss: 0.5721912980079651\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 70, Loss: 0.556165337562561\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 71, Loss: 0.5548055171966553\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 72, Loss: 0.5617607235908508\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 73, Loss: 0.5556647181510925\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 74, Loss: 0.5584151744842529\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 75, Loss: 0.566608726978302\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 76, Loss: 0.5578975081443787\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 77, Loss: 0.559209406375885\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 78, Loss: 0.5535319447517395\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 79, Loss: 0.5505171418190002\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 80, Loss: 0.5469560623168945\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 81, Loss: 0.5502329468727112\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 82, Loss: 0.5554229021072388\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 83, Loss: 0.5491636395454407\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 84, Loss: 0.5510196089744568\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 85, Loss: 0.5515267848968506\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 86, Loss: 0.5442143082618713\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 87, Loss: 0.5450311899185181\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 88, Loss: 0.5520883202552795\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 89, Loss: 0.5457998514175415\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 90, Loss: 0.5451249480247498\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 91, Loss: 0.5471499562263489\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 92, Loss: 0.5456022024154663\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 93, Loss: 0.5477027893066406\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 94, Loss: 0.5460208058357239\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 95, Loss: 0.5444580912590027\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 96, Loss: 0.5393815040588379\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 97, Loss: 0.5439400672912598\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 98, Loss: 0.5437869429588318\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 99, Loss: 0.5439914464950562\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 100, Loss: 0.5347018837928772\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 101, Loss: 0.5439978241920471\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 102, Loss: 0.5358182787895203\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 103, Loss: 0.5436432957649231\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 104, Loss: 0.5391460061073303\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 105, Loss: 0.5359081625938416\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 106, Loss: 0.5289304852485657\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 107, Loss: 0.5414831042289734\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 108, Loss: 0.5318456292152405\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 109, Loss: 0.5352426767349243\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 110, Loss: 0.5457814931869507\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 111, Loss: 0.5520833134651184\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 112, Loss: 0.539463460445404\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 113, Loss: 0.5426055788993835\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 114, Loss: 0.5375840067863464\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 115, Loss: 0.536998987197876\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 116, Loss: 0.5383788347244263\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 117, Loss: 0.5381180644035339\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 118, Loss: 0.525050163269043\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 119, Loss: 0.5340508222579956\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 120, Loss: 0.5349554419517517\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 121, Loss: 0.5312218070030212\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 122, Loss: 0.5374069213867188\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 123, Loss: 0.5353356003761292\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 124, Loss: 0.5327969193458557\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 125, Loss: 0.5313209295272827\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 126, Loss: 0.5284414887428284\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 127, Loss: 0.5385382175445557\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 128, Loss: 0.5326134562492371\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 129, Loss: 0.5292506217956543\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 130, Loss: 0.5323078036308289\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 131, Loss: 0.5283764600753784\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 132, Loss: 0.528176486492157\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 133, Loss: 0.5312318801879883\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 134, Loss: 0.5243510007858276\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 135, Loss: 0.5259433388710022\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 136, Loss: 0.5255509614944458\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 137, Loss: 0.5275842547416687\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 138, Loss: 0.5201215147972107\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 139, Loss: 0.5300716757774353\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 140, Loss: 0.5284309387207031\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 141, Loss: 0.519554078578949\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 142, Loss: 0.5217389464378357\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 143, Loss: 0.5233797430992126\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 144, Loss: 0.5252659916877747\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 145, Loss: 0.5253974199295044\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 146, Loss: 0.5202894806861877\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 147, Loss: 0.5125119090080261\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 148, Loss: 0.5217218995094299\n",
      "Das Pareto-Prinzip trifft nicht zu.\n",
      "Epoch: 149, Loss: 0.5239781737327576\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "\n",
    "weight_class_0 = 1.0\n",
    "weight_class_1 = 5.0\n",
    "class_weights = torch.Tensor([weight_class_0, weight_class_1])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def calculate_importance(h):\n",
    "    # Implementieren Sie Ihre Metrik zur Berechnung der Wichtigkeit hier.\n",
    "    # Hier verwenden wir einfach die L2-Norm der Hidden-ZustÃ¤nde als Beispiel.\n",
    "    importance = torch.norm(h, dim=1)\n",
    "    return importance\n",
    "\n",
    "def check_pareto(importance):\n",
    "    # Sortieren Sie die Wichtigkeit absteigend.\n",
    "    sorted_indices = torch.argsort(importance, descending=True)\n",
    "    cumulative_sum = torch.cumsum(importance[sorted_indices], dim=0)\n",
    "    threshold = 0.8 * cumulative_sum[-1]\n",
    "\n",
    "    # Finden Sie die Anzahl der Elemente, die das Pareto-Prinzip erfÃ¼llen.\n",
    "    pareto_elements = (cumulative_sum >= threshold).nonzero()\n",
    "\n",
    "    # ÃberprÃ¼fen Sie, ob das Pareto-Prinzip zutrifft und geben Sie die Ergebnisse aus.\n",
    "    if len(pareto_elements) <= 0.2 * len(importance):\n",
    "        print(\"Das Pareto-Prinzip trifft zu.\")\n",
    "    else:\n",
    "        print(\"Das Pareto-Prinzip trifft nicht zu.\")\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    # Berechnen Sie die Wichtigkeit der Elemente basierend auf den Hidden-ZustÃ¤nden.\n",
    "    importance = calculate_importance(h)\n",
    "\n",
    "    # ÃberprÃ¼fen Sie das Pareto-Prinzip.\n",
    "    check_pareto(importance)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss, h = train(usu_graph)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
