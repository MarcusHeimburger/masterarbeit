{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc, recall_score, precision_score\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Berechnet Accuracy Score\n",
    "def evaluateAccuracy(mymodel, data):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        out, _ = mymodel(data.x, data.edge_index)\n",
    "        predicted_labels = out.argmax(dim=1)\n",
    "    return accuracy_score(data.y[data.test_mask].numpy(), predicted_labels[data.test_mask].numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Berechnet AUC ROC\n",
    "def evaluateAUCROC(mymodel, data):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        out, _ = mymodel(data.x, data.edge_index)\n",
    "        predicted_labels = out[:, 1].cpu().numpy()\n",
    "        true_labels = data.y.numpy()\n",
    "        fpr, tpr, _ = roc_curve(true_labels[data.test_mask], predicted_labels[data.test_mask])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc, fpr, tpr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Berechnet Recall\n",
    "def evaluateRecall(mymodel, data):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        out, _ = mymodel(data.x, data.edge_index)\n",
    "        predicted_labels = out.argmax(dim=1)\n",
    "\n",
    "    ground_truth = data.y[data.test_mask].numpy()\n",
    "    predicted_labels = predicted_labels[data.test_mask].numpy()\n",
    "\n",
    "    recall = recall_score(ground_truth, predicted_labels)\n",
    "    return recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Berechnet Precision\n",
    "def evaluatePrecision(mymodel, data):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        out, _ = mymodel(data.x, data.edge_index)\n",
    "        predicted_labels = out.argmax(dim=1)\n",
    "\n",
    "    ground_truth = data.y[data.test_mask].numpy()\n",
    "    predicted_labels = predicted_labels[data.test_mask].numpy()\n",
    "\n",
    "    precision = precision_score(ground_truth, predicted_labels)\n",
    "    return precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Berechnet F1 Score\n",
    "def evaluateF1(mymodel):\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        out, _ = mymodel(usu_graph.x, usu_graph.edge_index)\n",
    "        predicted_labels = out.argmax(dim=1).cpu().numpy()\n",
    "    true_labels = usu_graph.y.numpy()\n",
    "    f1 = f1_score(true_labels[usu_graph.test_mask], predicted_labels[usu_graph.test_mask], average='macro')\n",
    "    return f1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Lädt Daten\n",
    "usu_graph = torch.load('Graph/usu_graph.pth')\n",
    "usu_graph_oversampled_easy = torch.load('Graph/syntetic_data.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: https://arxiv.org/pdf/2005.10150.pdf Table 1\n",
    "- Number of rated products\n",
    "- Length of username\n",
    "- Number and ratio of each rating level given by a user\n",
    "- Ratio of positive and negative ratings\n",
    "- Entropy of ratings\n",
    "- Total number of helpful and unhelpful votes a user gets\n",
    "- The ratio and mean of helpful and unhelpful votes\n",
    "- Median, min, and max number of helpful and unhelpful votes\n",
    "- Day gap\n",
    "- Time entropy\n",
    "- Same date indicator\n",
    "- Median, min, max, and average of ratings\n",
    "- Feedback summary length\n",
    "- Review text sentiment\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(25, 4)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph Convolutional Network\n",
    "Features: 25, Ausgabe: 2\n",
    "out: endgültige Vorhersage des Modells, h: Darstellung des Graphen nach den Transformationen durch die GCN-Schichten ist\n",
    "\"\"\"\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(25, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()\n",
    "\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9165054559707642\n",
      "Epoch: 1, Loss: 0.8757396936416626\n",
      "Epoch: 2, Loss: 0.837192177772522\n",
      "Epoch: 3, Loss: 0.8007850050926208\n",
      "Epoch: 4, Loss: 0.7663958072662354\n",
      "Epoch: 5, Loss: 0.7338666319847107\n",
      "Epoch: 6, Loss: 0.7031211256980896\n",
      "Epoch: 7, Loss: 0.6740131378173828\n",
      "Epoch: 8, Loss: 0.6463943123817444\n",
      "Epoch: 9, Loss: 0.6201349496841431\n",
      "Epoch: 10, Loss: 0.595116376876831\n",
      "Epoch: 11, Loss: 0.5712305903434753\n",
      "Epoch: 12, Loss: 0.5483885407447815\n",
      "Epoch: 13, Loss: 0.5265418291091919\n",
      "Epoch: 14, Loss: 0.5056853890419006\n",
      "Epoch: 15, Loss: 0.4857933521270752\n",
      "Epoch: 16, Loss: 0.46685484051704407\n",
      "Epoch: 17, Loss: 0.4488848149776459\n",
      "Epoch: 18, Loss: 0.4318750202655792\n",
      "Epoch: 19, Loss: 0.4158582389354706\n",
      "Epoch: 20, Loss: 0.40082937479019165\n",
      "Epoch: 21, Loss: 0.38678890466690063\n",
      "Epoch: 22, Loss: 0.37373122572898865\n",
      "Epoch: 23, Loss: 0.3616305887699127\n",
      "Epoch: 24, Loss: 0.3504321575164795\n",
      "Epoch: 25, Loss: 0.3401104509830475\n",
      "Epoch: 26, Loss: 0.3306248188018799\n",
      "Epoch: 27, Loss: 0.32194024324417114\n",
      "Epoch: 28, Loss: 0.314019113779068\n",
      "Epoch: 29, Loss: 0.30680403113365173\n",
      "Epoch: 30, Loss: 0.30031758546829224\n",
      "Epoch: 31, Loss: 0.2948499023914337\n",
      "Epoch: 32, Loss: 0.2897097170352936\n",
      "Epoch: 33, Loss: 0.28493791818618774\n",
      "Epoch: 34, Loss: 0.2805919349193573\n",
      "Epoch: 35, Loss: 0.27665746212005615\n",
      "Epoch: 36, Loss: 0.2731046974658966\n",
      "Epoch: 37, Loss: 0.2698962986469269\n",
      "Epoch: 38, Loss: 0.2670016288757324\n",
      "Epoch: 39, Loss: 0.26439499855041504\n",
      "Epoch: 40, Loss: 0.26205381751060486\n",
      "Epoch: 41, Loss: 0.2599564790725708\n",
      "Epoch: 42, Loss: 0.2580798268318176\n",
      "Epoch: 43, Loss: 0.2564019560813904\n",
      "Epoch: 44, Loss: 0.2549028992652893\n",
      "Epoch: 45, Loss: 0.25356459617614746\n",
      "Epoch: 46, Loss: 0.25237125158309937\n",
      "Epoch: 47, Loss: 0.25130757689476013\n",
      "Epoch: 48, Loss: 0.25035861134529114\n",
      "Epoch: 49, Loss: 0.24951118230819702\n",
      "Epoch: 50, Loss: 0.24875569343566895\n",
      "Epoch: 51, Loss: 0.24808543920516968\n",
      "Epoch: 52, Loss: 0.2474961280822754\n",
      "Epoch: 53, Loss: 0.24698294699192047\n",
      "Epoch: 54, Loss: 0.2465355396270752\n",
      "Epoch: 55, Loss: 0.2461431622505188\n",
      "Epoch: 56, Loss: 0.24579712748527527\n",
      "Epoch: 57, Loss: 0.2454916387796402\n",
      "Epoch: 58, Loss: 0.24522200226783752\n",
      "Epoch: 59, Loss: 0.24498391151428223\n",
      "Epoch: 60, Loss: 0.24477361142635345\n",
      "Epoch: 61, Loss: 0.24458803236484528\n",
      "Epoch: 62, Loss: 0.24442437291145325\n",
      "Epoch: 63, Loss: 0.24428017437458038\n",
      "Epoch: 64, Loss: 0.24415332078933716\n",
      "Epoch: 65, Loss: 0.24404186010360718\n",
      "Epoch: 66, Loss: 0.2439439743757248\n",
      "Epoch: 67, Loss: 0.2438579499721527\n",
      "Epoch: 68, Loss: 0.24378246068954468\n",
      "Epoch: 69, Loss: 0.24371632933616638\n",
      "Epoch: 70, Loss: 0.24365897476673126\n",
      "Epoch: 71, Loss: 0.24360917508602142\n",
      "Epoch: 72, Loss: 0.24356608092784882\n",
      "Epoch: 73, Loss: 0.24352894723415375\n",
      "Epoch: 74, Loss: 0.243496835231781\n",
      "Epoch: 75, Loss: 0.24346886575222015\n",
      "Epoch: 76, Loss: 0.24344424903392792\n",
      "Epoch: 77, Loss: 0.24342241883277893\n",
      "Epoch: 78, Loss: 0.24340291321277618\n",
      "Epoch: 79, Loss: 0.24338550865650177\n",
      "Epoch: 80, Loss: 0.24336977303028107\n",
      "Epoch: 81, Loss: 0.24335555732250214\n",
      "Epoch: 82, Loss: 0.2433425784111023\n",
      "Epoch: 83, Loss: 0.2433306723833084\n",
      "Epoch: 84, Loss: 0.24331960082054138\n",
      "Epoch: 85, Loss: 0.24330928921699524\n",
      "Epoch: 86, Loss: 0.24329955875873566\n",
      "Epoch: 87, Loss: 0.24329029023647308\n",
      "Epoch: 88, Loss: 0.24328140914440155\n",
      "Epoch: 89, Loss: 0.2432728111743927\n",
      "Epoch: 90, Loss: 0.24326445162296295\n",
      "Epoch: 91, Loss: 0.24325622618198395\n",
      "Epoch: 92, Loss: 0.24324814975261688\n",
      "Epoch: 93, Loss: 0.243240088224411\n",
      "Epoch: 94, Loss: 0.24323202669620514\n",
      "Epoch: 95, Loss: 0.24322399497032166\n",
      "Epoch: 96, Loss: 0.2432159185409546\n",
      "Epoch: 97, Loss: 0.24320784211158752\n",
      "Epoch: 98, Loss: 0.24319970607757568\n",
      "Epoch: 99, Loss: 0.24319154024124146\n",
      "Epoch: 100, Loss: 0.2431832104921341\n",
      "Epoch: 101, Loss: 0.24317486584186554\n",
      "Epoch: 102, Loss: 0.24316635727882385\n",
      "Epoch: 103, Loss: 0.2431577444076538\n",
      "Epoch: 104, Loss: 0.24314895272254944\n",
      "Epoch: 105, Loss: 0.2431400865316391\n",
      "Epoch: 106, Loss: 0.24313104152679443\n",
      "Epoch: 107, Loss: 0.24312178790569305\n",
      "Epoch: 108, Loss: 0.24311241507530212\n",
      "Epoch: 109, Loss: 0.24310284852981567\n",
      "Epoch: 110, Loss: 0.24309314787387848\n",
      "Epoch: 111, Loss: 0.24308320879936218\n",
      "Epoch: 112, Loss: 0.2430730015039444\n",
      "Epoch: 113, Loss: 0.2430625557899475\n",
      "Epoch: 114, Loss: 0.24305175244808197\n",
      "Epoch: 115, Loss: 0.2430405467748642\n",
      "Epoch: 116, Loss: 0.24302875995635986\n",
      "Epoch: 117, Loss: 0.24301619827747345\n",
      "Epoch: 118, Loss: 0.2430025339126587\n",
      "Epoch: 119, Loss: 0.24298730492591858\n",
      "Epoch: 120, Loss: 0.24296963214874268\n",
      "Epoch: 121, Loss: 0.2429487705230713\n",
      "Epoch: 122, Loss: 0.24292433261871338\n",
      "Epoch: 123, Loss: 0.242897167801857\n",
      "Epoch: 124, Loss: 0.242868110537529\n",
      "Epoch: 125, Loss: 0.2428365796804428\n",
      "Epoch: 126, Loss: 0.2428009808063507\n",
      "Epoch: 127, Loss: 0.2427578717470169\n",
      "Epoch: 128, Loss: 0.2427024096250534\n",
      "Epoch: 129, Loss: 0.2426334172487259\n",
      "Epoch: 130, Loss: 0.24256488680839539\n",
      "Epoch: 131, Loss: 0.24250546097755432\n",
      "Epoch: 132, Loss: 0.2424384355545044\n",
      "Epoch: 133, Loss: 0.24235738813877106\n",
      "Epoch: 134, Loss: 0.2422924041748047\n",
      "Epoch: 135, Loss: 0.24221199750900269\n",
      "Epoch: 136, Loss: 0.24205555021762848\n",
      "Epoch: 137, Loss: 0.2417832314968109\n",
      "Epoch: 138, Loss: 0.2412341982126236\n",
      "Epoch: 139, Loss: 0.24044528603553772\n",
      "Epoch: 140, Loss: 0.23943178355693817\n",
      "Epoch: 141, Loss: 0.2386324107646942\n",
      "Epoch: 142, Loss: 0.23784178495407104\n",
      "Epoch: 143, Loss: 0.23744530975818634\n",
      "Epoch: 144, Loss: 0.24007248878479004\n",
      "Epoch: 145, Loss: 0.2371131330728531\n",
      "Epoch: 146, Loss: 0.23729729652404785\n",
      "Epoch: 147, Loss: 0.2374708503484726\n",
      "Epoch: 148, Loss: 0.23756296932697296\n",
      "Epoch: 149, Loss: 0.23762860894203186\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss, h = train(usu_graph)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "torch.save(model, 'Models/standard.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gewichtete Verlustfunktion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(25, 4)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph Convolutional Network\n",
    "drei GCN-Schichten (GCNConv) und eine lineare Schicht (Linear)\n",
    "Features: 25, Ausgabe: 2\n",
    "out: endgültige Vorhersage des Modells, h: Darstellung des Graphen nach den Transformationen durch die GCN-Schichten ist\n",
    "\"\"\"\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(25, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()\n",
    "\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8211929202079773\n",
      "Epoch: 1, Loss: 0.7950437664985657\n",
      "Epoch: 2, Loss: 0.770831286907196\n",
      "Epoch: 3, Loss: 0.7484997510910034\n",
      "Epoch: 4, Loss: 0.7279432415962219\n",
      "Epoch: 5, Loss: 0.7090468406677246\n",
      "Epoch: 6, Loss: 0.6917185187339783\n",
      "Epoch: 7, Loss: 0.6758506298065186\n",
      "Epoch: 8, Loss: 0.661325991153717\n",
      "Epoch: 9, Loss: 0.648041307926178\n",
      "Epoch: 10, Loss: 0.6359065175056458\n",
      "Epoch: 11, Loss: 0.6248509883880615\n",
      "Epoch: 12, Loss: 0.6148212552070618\n",
      "Epoch: 13, Loss: 0.6057900190353394\n",
      "Epoch: 14, Loss: 0.5977352857589722\n",
      "Epoch: 15, Loss: 0.5906287431716919\n",
      "Epoch: 16, Loss: 0.5844376683235168\n",
      "Epoch: 17, Loss: 0.5791180729866028\n",
      "Epoch: 18, Loss: 0.5745773911476135\n",
      "Epoch: 19, Loss: 0.5707274675369263\n",
      "Epoch: 20, Loss: 0.5674394965171814\n",
      "Epoch: 21, Loss: 0.5645534992218018\n",
      "Epoch: 22, Loss: 0.5619769096374512\n",
      "Epoch: 23, Loss: 0.5596712827682495\n",
      "Epoch: 24, Loss: 0.5574339032173157\n",
      "Epoch: 25, Loss: 0.5553075671195984\n",
      "Epoch: 26, Loss: 0.5534754991531372\n",
      "Epoch: 27, Loss: 0.5516083836555481\n",
      "Epoch: 28, Loss: 0.5492284893989563\n",
      "Epoch: 29, Loss: 0.5458797216415405\n",
      "Epoch: 30, Loss: 0.5458405017852783\n",
      "Epoch: 31, Loss: 0.5415960550308228\n",
      "Epoch: 32, Loss: 0.5408148169517517\n",
      "Epoch: 33, Loss: 0.5395548343658447\n",
      "Epoch: 34, Loss: 0.5355201959609985\n",
      "Epoch: 35, Loss: 0.6642615795135498\n",
      "Epoch: 36, Loss: 0.5437629222869873\n",
      "Epoch: 37, Loss: 0.5389603972434998\n",
      "Epoch: 38, Loss: 0.5414689779281616\n",
      "Epoch: 39, Loss: 0.5429184436798096\n",
      "Epoch: 40, Loss: 0.5438023209571838\n",
      "Epoch: 41, Loss: 0.544256865978241\n",
      "Epoch: 42, Loss: 0.5442764759063721\n",
      "Epoch: 43, Loss: 0.5439168214797974\n",
      "Epoch: 44, Loss: 0.5432367324829102\n",
      "Epoch: 45, Loss: 0.5422871112823486\n",
      "Epoch: 46, Loss: 0.5411210060119629\n",
      "Epoch: 47, Loss: 0.5397883653640747\n",
      "Epoch: 48, Loss: 0.5383451581001282\n",
      "Epoch: 49, Loss: 0.5368484854698181\n",
      "Epoch: 50, Loss: 0.5353593230247498\n",
      "Epoch: 51, Loss: 0.5339449644088745\n",
      "Epoch: 52, Loss: 0.532676100730896\n",
      "Epoch: 53, Loss: 0.5316087007522583\n",
      "Epoch: 54, Loss: 0.5307795405387878\n",
      "Epoch: 55, Loss: 0.5302223563194275\n",
      "Epoch: 56, Loss: 0.529941737651825\n",
      "Epoch: 57, Loss: 0.5298808813095093\n",
      "Epoch: 58, Loss: 0.5299205183982849\n",
      "Epoch: 59, Loss: 0.5299105644226074\n",
      "Epoch: 60, Loss: 0.5297165513038635\n",
      "Epoch: 61, Loss: 0.5292618274688721\n",
      "Epoch: 62, Loss: 0.5285499691963196\n",
      "Epoch: 63, Loss: 0.5276577472686768\n",
      "Epoch: 64, Loss: 0.5267013311386108\n",
      "Epoch: 65, Loss: 0.5257953405380249\n",
      "Epoch: 66, Loss: 0.5250218510627747\n",
      "Epoch: 67, Loss: 0.5244143605232239\n",
      "Epoch: 68, Loss: 0.5239585041999817\n",
      "Epoch: 69, Loss: 0.5236092805862427\n",
      "Epoch: 70, Loss: 0.5233099460601807\n",
      "Epoch: 71, Loss: 0.5230070948600769\n",
      "Epoch: 72, Loss: 0.5226616263389587\n",
      "Epoch: 73, Loss: 0.5222533345222473\n",
      "Epoch: 74, Loss: 0.5217831134796143\n",
      "Epoch: 75, Loss: 0.5212698578834534\n",
      "Epoch: 76, Loss: 0.5207462310791016\n",
      "Epoch: 77, Loss: 0.5202487111091614\n",
      "Epoch: 78, Loss: 0.5198079347610474\n",
      "Epoch: 79, Loss: 0.5194383263587952\n",
      "Epoch: 80, Loss: 0.5191323161125183\n",
      "Epoch: 81, Loss: 0.5188635587692261\n",
      "Epoch: 82, Loss: 0.5185979008674622\n",
      "Epoch: 83, Loss: 0.5183065533638\n",
      "Epoch: 84, Loss: 0.5179762840270996\n",
      "Epoch: 85, Loss: 0.5176129341125488\n",
      "Epoch: 86, Loss: 0.5172364115715027\n",
      "Epoch: 87, Loss: 0.5168709754943848\n",
      "Epoch: 88, Loss: 0.5165336728096008\n",
      "Epoch: 89, Loss: 0.5162283778190613\n",
      "Epoch: 90, Loss: 0.5159475803375244\n",
      "Epoch: 91, Loss: 0.5156781673431396\n",
      "Epoch: 92, Loss: 0.5154069066047668\n",
      "Epoch: 93, Loss: 0.5151258111000061\n",
      "Epoch: 94, Loss: 0.5148330330848694\n",
      "Epoch: 95, Loss: 0.5145325660705566\n",
      "Epoch: 96, Loss: 0.5142309665679932\n",
      "Epoch: 97, Loss: 0.513934850692749\n",
      "Epoch: 98, Loss: 0.5136483907699585\n",
      "Epoch: 99, Loss: 0.5133711695671082\n",
      "Epoch: 100, Loss: 0.513099730014801\n",
      "Epoch: 101, Loss: 0.5128277540206909\n",
      "Epoch: 102, Loss: 0.5125493407249451\n",
      "Epoch: 103, Loss: 0.5122608542442322\n",
      "Epoch: 104, Loss: 0.5119622945785522\n",
      "Epoch: 105, Loss: 0.5116574168205261\n",
      "Epoch: 106, Loss: 0.5113518238067627\n",
      "Epoch: 107, Loss: 0.5110505223274231\n",
      "Epoch: 108, Loss: 0.5107558965682983\n",
      "Epoch: 109, Loss: 0.510467529296875\n",
      "Epoch: 110, Loss: 0.5101827383041382\n",
      "Epoch: 111, Loss: 0.50989830493927\n",
      "Epoch: 112, Loss: 0.509611964225769\n",
      "Epoch: 113, Loss: 0.5093234777450562\n",
      "Epoch: 114, Loss: 0.5090346932411194\n",
      "Epoch: 115, Loss: 0.5087475180625916\n",
      "Epoch: 116, Loss: 0.5084633231163025\n",
      "Epoch: 117, Loss: 0.5081814527511597\n",
      "Epoch: 118, Loss: 0.5078999400138855\n",
      "Epoch: 119, Loss: 0.5076161623001099\n",
      "Epoch: 120, Loss: 0.5073275566101074\n",
      "Epoch: 121, Loss: 0.5070331692695618\n",
      "Epoch: 122, Loss: 0.5067328810691833\n",
      "Epoch: 123, Loss: 0.5064277648925781\n",
      "Epoch: 124, Loss: 0.5061190724372864\n",
      "Epoch: 125, Loss: 0.5058072209358215\n",
      "Epoch: 126, Loss: 0.5054923295974731\n",
      "Epoch: 127, Loss: 0.5051735639572144\n",
      "Epoch: 128, Loss: 0.5048502683639526\n",
      "Epoch: 129, Loss: 0.5045220255851746\n",
      "Epoch: 130, Loss: 0.5041888356208801\n",
      "Epoch: 131, Loss: 0.5038518309593201\n",
      "Epoch: 132, Loss: 0.5035120844841003\n",
      "Epoch: 133, Loss: 0.5031709671020508\n",
      "Epoch: 134, Loss: 0.5028287172317505\n",
      "Epoch: 135, Loss: 0.5024852156639099\n",
      "Epoch: 136, Loss: 0.5021398067474365\n",
      "Epoch: 137, Loss: 0.5017921924591064\n",
      "Epoch: 138, Loss: 0.5014422535896301\n",
      "Epoch: 139, Loss: 0.5010903477668762\n",
      "Epoch: 140, Loss: 0.5007368922233582\n",
      "Epoch: 141, Loss: 0.5003820657730103\n",
      "Epoch: 142, Loss: 0.5000262260437012\n",
      "Epoch: 143, Loss: 0.49967002868652344\n",
      "Epoch: 144, Loss: 0.4993148446083069\n",
      "Epoch: 145, Loss: 0.4989621341228485\n",
      "Epoch: 146, Loss: 0.4986130893230438\n",
      "Epoch: 147, Loss: 0.4982684254646301\n",
      "Epoch: 148, Loss: 0.497928261756897\n",
      "Epoch: 149, Loss: 0.49759212136268616\n"
     ]
    }
   ],
   "source": [
    "# gewichtete Trainingsfunktion\n",
    "model = GCN()\n",
    "\n",
    "weight_class_0 = 1.0\n",
    "weight_class_1 = 5.0\n",
    "class_weights = torch.Tensor([weight_class_0, weight_class_1])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss, h = train(usu_graph)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "torch.save(model, 'Models/gewichteteLoss.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dropout Layer und Batch Normalize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(25, 4)\n",
      "  (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (bn3): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(25, 4)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout1 = torch.nn.Dropout(0.5)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(2)\n",
    "        self.dropout3 = torch.nn.Dropout(0.5)\n",
    "        self.classifier = Linear(2, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.bn1(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout1(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.bn2(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout2(h)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self.bn3(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout3(h)\n",
    "\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9451559782028198\n",
      "Epoch: 1, Loss: 0.9339910745620728\n",
      "Epoch: 2, Loss: 0.9260878562927246\n",
      "Epoch: 3, Loss: 0.9077959656715393\n",
      "Epoch: 4, Loss: 0.8949931859970093\n",
      "Epoch: 5, Loss: 0.8849787712097168\n",
      "Epoch: 6, Loss: 0.8755815029144287\n",
      "Epoch: 7, Loss: 0.8649537563323975\n",
      "Epoch: 8, Loss: 0.8592509627342224\n",
      "Epoch: 9, Loss: 0.8498400449752808\n",
      "Epoch: 10, Loss: 0.8405616879463196\n",
      "Epoch: 11, Loss: 0.8331317901611328\n",
      "Epoch: 12, Loss: 0.8191952109336853\n",
      "Epoch: 13, Loss: 0.8127669095993042\n",
      "Epoch: 14, Loss: 0.7983057498931885\n",
      "Epoch: 15, Loss: 0.7955847382545471\n",
      "Epoch: 16, Loss: 0.7840031981468201\n",
      "Epoch: 17, Loss: 0.7748575806617737\n",
      "Epoch: 18, Loss: 0.7645233273506165\n",
      "Epoch: 19, Loss: 0.760486364364624\n",
      "Epoch: 20, Loss: 0.7463226914405823\n",
      "Epoch: 21, Loss: 0.7413080930709839\n",
      "Epoch: 22, Loss: 0.7297562956809998\n",
      "Epoch: 23, Loss: 0.7228385210037231\n",
      "Epoch: 24, Loss: 0.710679292678833\n",
      "Epoch: 25, Loss: 0.7073660492897034\n",
      "Epoch: 26, Loss: 0.6974738836288452\n",
      "Epoch: 27, Loss: 0.6854221224784851\n",
      "Epoch: 28, Loss: 0.6809446811676025\n",
      "Epoch: 29, Loss: 0.6710963845252991\n",
      "Epoch: 30, Loss: 0.6726298928260803\n",
      "Epoch: 31, Loss: 0.6603862643241882\n",
      "Epoch: 32, Loss: 0.6525924801826477\n",
      "Epoch: 33, Loss: 0.6474714875221252\n",
      "Epoch: 34, Loss: 0.6514859199523926\n",
      "Epoch: 35, Loss: 0.6349793672561646\n",
      "Epoch: 36, Loss: 0.63369220495224\n",
      "Epoch: 37, Loss: 0.6280792951583862\n",
      "Epoch: 38, Loss: 0.619986891746521\n",
      "Epoch: 39, Loss: 0.6106069087982178\n",
      "Epoch: 40, Loss: 0.616296648979187\n",
      "Epoch: 41, Loss: 0.6131098866462708\n",
      "Epoch: 42, Loss: 0.6009458899497986\n",
      "Epoch: 43, Loss: 0.6025345921516418\n",
      "Epoch: 44, Loss: 0.589739978313446\n",
      "Epoch: 45, Loss: 0.5929201245307922\n",
      "Epoch: 46, Loss: 0.5841706395149231\n",
      "Epoch: 47, Loss: 0.6119787693023682\n",
      "Epoch: 48, Loss: 0.5833016037940979\n",
      "Epoch: 49, Loss: 0.5809698104858398\n",
      "Epoch: 50, Loss: 0.5849145650863647\n",
      "Epoch: 51, Loss: 0.5761037468910217\n",
      "Epoch: 52, Loss: 0.5711724758148193\n",
      "Epoch: 53, Loss: 0.5781532526016235\n",
      "Epoch: 54, Loss: 0.5706735849380493\n",
      "Epoch: 55, Loss: 0.5715283155441284\n",
      "Epoch: 56, Loss: 0.5693480968475342\n",
      "Epoch: 57, Loss: 0.569870114326477\n",
      "Epoch: 58, Loss: 0.5692561864852905\n",
      "Epoch: 59, Loss: 0.5622370839118958\n",
      "Epoch: 60, Loss: 0.5635805726051331\n",
      "Epoch: 61, Loss: 0.5631416440010071\n",
      "Epoch: 62, Loss: 0.5727682113647461\n",
      "Epoch: 63, Loss: 0.567432701587677\n",
      "Epoch: 64, Loss: 0.5664543509483337\n",
      "Epoch: 65, Loss: 0.5677257180213928\n",
      "Epoch: 66, Loss: 0.5600593686103821\n",
      "Epoch: 67, Loss: 0.558536946773529\n",
      "Epoch: 68, Loss: 0.5603164434432983\n",
      "Epoch: 69, Loss: 0.5721912980079651\n",
      "Epoch: 70, Loss: 0.556165337562561\n",
      "Epoch: 71, Loss: 0.5548055171966553\n",
      "Epoch: 72, Loss: 0.5617607235908508\n",
      "Epoch: 73, Loss: 0.5556647181510925\n",
      "Epoch: 74, Loss: 0.5584151744842529\n",
      "Epoch: 75, Loss: 0.566608726978302\n",
      "Epoch: 76, Loss: 0.5578975081443787\n",
      "Epoch: 77, Loss: 0.559209406375885\n",
      "Epoch: 78, Loss: 0.5535319447517395\n",
      "Epoch: 79, Loss: 0.5505171418190002\n",
      "Epoch: 80, Loss: 0.5469560623168945\n",
      "Epoch: 81, Loss: 0.5502329468727112\n",
      "Epoch: 82, Loss: 0.5554229021072388\n",
      "Epoch: 83, Loss: 0.5491636395454407\n",
      "Epoch: 84, Loss: 0.5510196089744568\n",
      "Epoch: 85, Loss: 0.5515267848968506\n",
      "Epoch: 86, Loss: 0.5442143082618713\n",
      "Epoch: 87, Loss: 0.5450311899185181\n",
      "Epoch: 88, Loss: 0.5520883202552795\n",
      "Epoch: 89, Loss: 0.5457998514175415\n",
      "Epoch: 90, Loss: 0.5451249480247498\n",
      "Epoch: 91, Loss: 0.5471499562263489\n",
      "Epoch: 92, Loss: 0.5456022024154663\n",
      "Epoch: 93, Loss: 0.5477027893066406\n",
      "Epoch: 94, Loss: 0.5460208058357239\n",
      "Epoch: 95, Loss: 0.5444580912590027\n",
      "Epoch: 96, Loss: 0.5393815040588379\n",
      "Epoch: 97, Loss: 0.5439400672912598\n",
      "Epoch: 98, Loss: 0.5437869429588318\n",
      "Epoch: 99, Loss: 0.5439914464950562\n",
      "Epoch: 100, Loss: 0.5347018837928772\n",
      "Epoch: 101, Loss: 0.5439978241920471\n",
      "Epoch: 102, Loss: 0.5358182787895203\n",
      "Epoch: 103, Loss: 0.5436432957649231\n",
      "Epoch: 104, Loss: 0.5391460061073303\n",
      "Epoch: 105, Loss: 0.5359081625938416\n",
      "Epoch: 106, Loss: 0.5289304852485657\n",
      "Epoch: 107, Loss: 0.5414831042289734\n",
      "Epoch: 108, Loss: 0.5318456292152405\n",
      "Epoch: 109, Loss: 0.5352426767349243\n",
      "Epoch: 110, Loss: 0.5457814931869507\n",
      "Epoch: 111, Loss: 0.5520833134651184\n",
      "Epoch: 112, Loss: 0.539463460445404\n",
      "Epoch: 113, Loss: 0.5426055788993835\n",
      "Epoch: 114, Loss: 0.5375840067863464\n",
      "Epoch: 115, Loss: 0.536998987197876\n",
      "Epoch: 116, Loss: 0.5383788347244263\n",
      "Epoch: 117, Loss: 0.5381180644035339\n",
      "Epoch: 118, Loss: 0.525050163269043\n",
      "Epoch: 119, Loss: 0.5340508222579956\n",
      "Epoch: 120, Loss: 0.5349554419517517\n",
      "Epoch: 121, Loss: 0.5312218070030212\n",
      "Epoch: 122, Loss: 0.5374069213867188\n",
      "Epoch: 123, Loss: 0.5353356003761292\n",
      "Epoch: 124, Loss: 0.5327969193458557\n",
      "Epoch: 125, Loss: 0.5313209295272827\n",
      "Epoch: 126, Loss: 0.5284414887428284\n",
      "Epoch: 127, Loss: 0.5385382175445557\n",
      "Epoch: 128, Loss: 0.5326134562492371\n",
      "Epoch: 129, Loss: 0.5292506217956543\n",
      "Epoch: 130, Loss: 0.5323078036308289\n",
      "Epoch: 131, Loss: 0.5283764600753784\n",
      "Epoch: 132, Loss: 0.528176486492157\n",
      "Epoch: 133, Loss: 0.5312318801879883\n",
      "Epoch: 134, Loss: 0.5243510007858276\n",
      "Epoch: 135, Loss: 0.5259433388710022\n",
      "Epoch: 136, Loss: 0.5255509614944458\n",
      "Epoch: 137, Loss: 0.5275842547416687\n",
      "Epoch: 138, Loss: 0.5201215147972107\n",
      "Epoch: 139, Loss: 0.5300716757774353\n",
      "Epoch: 140, Loss: 0.5284309387207031\n",
      "Epoch: 141, Loss: 0.519554078578949\n",
      "Epoch: 142, Loss: 0.5217389464378357\n",
      "Epoch: 143, Loss: 0.5233797430992126\n",
      "Epoch: 144, Loss: 0.5252659916877747\n",
      "Epoch: 145, Loss: 0.5253974199295044\n",
      "Epoch: 146, Loss: 0.5202894806861877\n",
      "Epoch: 147, Loss: 0.5125119090080261\n",
      "Epoch: 148, Loss: 0.5217218995094299\n",
      "Epoch: 149, Loss: 0.5239781737327576\n"
     ]
    }
   ],
   "source": [
    "# Trainingsfunktion\n",
    "model = GCN()\n",
    "\n",
    "weight_class_0 = 1.0\n",
    "weight_class_1 = 5.0\n",
    "class_weights = torch.Tensor([weight_class_0, weight_class_1])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss, h = train(usu_graph)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "torch.save(model, 'Models/batchNorm.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetischer Datensatz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(25, 4)\n",
      "  (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (bn3): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(25, 4)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout1 = torch.nn.Dropout(0.5)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(4)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(2)\n",
    "        self.dropout3 = torch.nn.Dropout(0.5)\n",
    "        self.classifier = Linear(2, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.bn1(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout1(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.bn2(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout2(h)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self.bn3(h)\n",
    "        h = h.tanh()\n",
    "        h = self.dropout3(h)\n",
    "\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.771479070186615\n",
      "Epoch: 1, Loss: 0.7718269228935242\n",
      "Epoch: 2, Loss: 0.7669517397880554\n",
      "Epoch: 3, Loss: 0.7550415992736816\n",
      "Epoch: 4, Loss: 0.7519038915634155\n",
      "Epoch: 5, Loss: 0.7423825263977051\n",
      "Epoch: 6, Loss: 0.7344754338264465\n",
      "Epoch: 7, Loss: 0.7359898686408997\n",
      "Epoch: 8, Loss: 0.732512891292572\n",
      "Epoch: 9, Loss: 0.7283227443695068\n",
      "Epoch: 10, Loss: 0.7198048233985901\n",
      "Epoch: 11, Loss: 0.7130639553070068\n",
      "Epoch: 12, Loss: 0.7096067070960999\n",
      "Epoch: 13, Loss: 0.7074815034866333\n",
      "Epoch: 14, Loss: 0.7006235718727112\n",
      "Epoch: 15, Loss: 0.6989489197731018\n",
      "Epoch: 16, Loss: 0.6980144381523132\n",
      "Epoch: 17, Loss: 0.6961043477058411\n",
      "Epoch: 18, Loss: 0.690750241279602\n",
      "Epoch: 19, Loss: 0.6870208978652954\n",
      "Epoch: 20, Loss: 0.6851810812950134\n",
      "Epoch: 21, Loss: 0.6837525963783264\n",
      "Epoch: 22, Loss: 0.6812517642974854\n",
      "Epoch: 23, Loss: 0.6806206107139587\n",
      "Epoch: 24, Loss: 0.6760877966880798\n",
      "Epoch: 25, Loss: 0.6825188994407654\n",
      "Epoch: 26, Loss: 0.6688545942306519\n",
      "Epoch: 27, Loss: 0.667036235332489\n",
      "Epoch: 28, Loss: 0.6726393103599548\n",
      "Epoch: 29, Loss: 0.6640289425849915\n",
      "Epoch: 30, Loss: 0.6709633469581604\n",
      "Epoch: 31, Loss: 0.6682158708572388\n",
      "Epoch: 32, Loss: 0.668147623538971\n",
      "Epoch: 33, Loss: 0.6655614376068115\n",
      "Epoch: 34, Loss: 0.6683946251869202\n",
      "Epoch: 35, Loss: 0.6577273011207581\n",
      "Epoch: 36, Loss: 0.6500264406204224\n",
      "Epoch: 37, Loss: 0.6526398658752441\n",
      "Epoch: 38, Loss: 0.6518415212631226\n",
      "Epoch: 39, Loss: 0.653221845626831\n",
      "Epoch: 40, Loss: 0.6551281213760376\n",
      "Epoch: 41, Loss: 0.6487919688224792\n",
      "Epoch: 42, Loss: 0.6407842636108398\n",
      "Epoch: 43, Loss: 0.6469369530677795\n",
      "Epoch: 44, Loss: 0.6370692253112793\n",
      "Epoch: 45, Loss: 0.6529422998428345\n",
      "Epoch: 46, Loss: 0.6403365135192871\n",
      "Epoch: 47, Loss: 0.6434139013290405\n",
      "Epoch: 48, Loss: 0.6312312483787537\n",
      "Epoch: 49, Loss: 0.6286686658859253\n",
      "Epoch: 50, Loss: 0.6306801438331604\n",
      "Epoch: 51, Loss: 0.6310805678367615\n",
      "Epoch: 52, Loss: 0.6253795027732849\n",
      "Epoch: 53, Loss: 0.6280861496925354\n",
      "Epoch: 54, Loss: 0.6276121735572815\n",
      "Epoch: 55, Loss: 0.6297460198402405\n",
      "Epoch: 56, Loss: 0.623096227645874\n",
      "Epoch: 57, Loss: 0.6153687834739685\n",
      "Epoch: 58, Loss: 0.619952917098999\n",
      "Epoch: 59, Loss: 0.6119462251663208\n",
      "Epoch: 60, Loss: 0.6273036599159241\n",
      "Epoch: 61, Loss: 0.6130456328392029\n",
      "Epoch: 62, Loss: 0.6289119124412537\n",
      "Epoch: 63, Loss: 0.6226896047592163\n",
      "Epoch: 64, Loss: 0.6104182004928589\n",
      "Epoch: 65, Loss: 0.612828254699707\n",
      "Epoch: 66, Loss: 0.6134384274482727\n",
      "Epoch: 67, Loss: 0.6109205484390259\n",
      "Epoch: 68, Loss: 0.6145214438438416\n",
      "Epoch: 69, Loss: 0.6160425543785095\n",
      "Epoch: 70, Loss: 0.6114106774330139\n",
      "Epoch: 71, Loss: 0.5996880531311035\n",
      "Epoch: 72, Loss: 0.5959459543228149\n",
      "Epoch: 73, Loss: 0.6045937538146973\n",
      "Epoch: 74, Loss: 0.6021363139152527\n",
      "Epoch: 75, Loss: 0.6018998026847839\n",
      "Epoch: 76, Loss: 0.5980134606361389\n",
      "Epoch: 77, Loss: 0.6015356779098511\n",
      "Epoch: 78, Loss: 0.5931439399719238\n",
      "Epoch: 79, Loss: 0.5883129835128784\n",
      "Epoch: 80, Loss: 0.5970785617828369\n",
      "Epoch: 81, Loss: 0.5874484181404114\n",
      "Epoch: 82, Loss: 0.5871594548225403\n",
      "Epoch: 83, Loss: 0.5875017046928406\n",
      "Epoch: 84, Loss: 0.5866237282752991\n",
      "Epoch: 85, Loss: 0.591161847114563\n",
      "Epoch: 86, Loss: 0.5884340405464172\n",
      "Epoch: 87, Loss: 0.578332781791687\n",
      "Epoch: 88, Loss: 0.580238938331604\n",
      "Epoch: 89, Loss: 0.576382577419281\n",
      "Epoch: 90, Loss: 0.5798771381378174\n",
      "Epoch: 91, Loss: 0.5775583982467651\n",
      "Epoch: 92, Loss: 0.5778743624687195\n",
      "Epoch: 93, Loss: 0.5841166973114014\n",
      "Epoch: 94, Loss: 0.5778717994689941\n",
      "Epoch: 95, Loss: 0.5730833411216736\n",
      "Epoch: 96, Loss: 0.5777337551116943\n",
      "Epoch: 97, Loss: 0.568418025970459\n",
      "Epoch: 98, Loss: 0.5760638117790222\n",
      "Epoch: 99, Loss: 0.5745478272438049\n",
      "Epoch: 100, Loss: 0.5741464495658875\n",
      "Epoch: 101, Loss: 0.5901422500610352\n",
      "Epoch: 102, Loss: 0.5772228837013245\n",
      "Epoch: 103, Loss: 0.5835124254226685\n",
      "Epoch: 104, Loss: 0.5812225341796875\n",
      "Epoch: 105, Loss: 0.5810099244117737\n",
      "Epoch: 106, Loss: 0.5743443369865417\n",
      "Epoch: 107, Loss: 0.5818177461624146\n",
      "Epoch: 108, Loss: 0.5812634825706482\n",
      "Epoch: 109, Loss: 0.5904809832572937\n",
      "Epoch: 110, Loss: 0.5826311111450195\n",
      "Epoch: 111, Loss: 0.5894122123718262\n",
      "Epoch: 112, Loss: 0.5693803429603577\n",
      "Epoch: 113, Loss: 0.5716919898986816\n",
      "Epoch: 114, Loss: 0.5856508016586304\n",
      "Epoch: 115, Loss: 0.5763757228851318\n",
      "Epoch: 116, Loss: 0.5718900561332703\n",
      "Epoch: 117, Loss: 0.5720483064651489\n",
      "Epoch: 118, Loss: 0.5649274587631226\n",
      "Epoch: 119, Loss: 0.5675473213195801\n",
      "Epoch: 120, Loss: 0.562730610370636\n",
      "Epoch: 121, Loss: 0.5743803381919861\n",
      "Epoch: 122, Loss: 0.5707970857620239\n",
      "Epoch: 123, Loss: 0.565447986125946\n",
      "Epoch: 124, Loss: 0.5736874341964722\n",
      "Epoch: 125, Loss: 0.5685245394706726\n",
      "Epoch: 126, Loss: 0.5657530426979065\n",
      "Epoch: 127, Loss: 0.5756242275238037\n",
      "Epoch: 128, Loss: 0.5643061399459839\n",
      "Epoch: 129, Loss: 0.5472210049629211\n",
      "Epoch: 130, Loss: 0.5655248761177063\n",
      "Epoch: 131, Loss: 0.5686999559402466\n",
      "Epoch: 132, Loss: 0.5606077313423157\n",
      "Epoch: 133, Loss: 0.5664249062538147\n",
      "Epoch: 134, Loss: 0.5598370432853699\n",
      "Epoch: 135, Loss: 0.560798168182373\n",
      "Epoch: 136, Loss: 0.5653461813926697\n",
      "Epoch: 137, Loss: 0.5615053176879883\n",
      "Epoch: 138, Loss: 0.5507078766822815\n",
      "Epoch: 139, Loss: 0.564978301525116\n",
      "Epoch: 140, Loss: 0.5627974271774292\n",
      "Epoch: 141, Loss: 0.5518472194671631\n",
      "Epoch: 142, Loss: 0.5529055595397949\n",
      "Epoch: 143, Loss: 0.5485391616821289\n",
      "Epoch: 144, Loss: 0.5503045320510864\n",
      "Epoch: 145, Loss: 0.5499845147132874\n",
      "Epoch: 146, Loss: 0.5483154654502869\n",
      "Epoch: 147, Loss: 0.5558682084083557\n",
      "Epoch: 148, Loss: 0.5513094663619995\n",
      "Epoch: 149, Loss: 0.555682897567749\n"
     ]
    }
   ],
   "source": [
    "# Trainigsfunktion\n",
    "model = GCN()\n",
    "\n",
    "weight_class_0 = 1.0\n",
    "weight_class_1 = 5.0\n",
    "class_weights = torch.Tensor([weight_class_0, weight_class_1])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss, h = train(usu_graph_oversampled_easy)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "torch.save(model, 'Models/synthetic.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
